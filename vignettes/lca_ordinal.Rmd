---
title: "Tutorial: Latent Class Analysis for Ordinal Indicators"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Tutorial: Latent Class Analysis for Ordinal Indicators}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE, cache = FALSE}
library(yaml)
library(scales)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE,
  warning = FALSE
)
options(scipen = 1, digits = 2)
run_everything = FALSE
```

This is an example of exploratory LCA with ordinal indicators using `tidySEM`.
The present example uses a simulated dataset with ordinal indicators included in `tidySEM`.
To view its documentation,
run the command `?tidySEM::data_mix_ordinal` in the R console.

## Loading the Data

To load the data, simply attach the `tidySEM` package.
For convenience, we assign the data to an object called `df`:

```{r, echo = TRUE, eval=TRUE}
# Load required packages
library(tidySEM) 
library(ggplot2)
# Load data
df <- data_mix_ordinal
```

## Examining the Data

As per the best practices,
the first step in LCA is examining the observed data.
We use `tidySEM::descriptives()` to describe the data numerically.
Because all items are categorical,
we remove columns for continuous data to de-clutter the table:

```{r tabdesc, echo = TRUE, eval=TRUE, results='asis'}
desc <- tidySEM::descriptives(df)
desc <- desc[, c("name", "type", "n", "missing", "unique", 
"mode", "mode_value", "v")
]
knitr::kable(desc, caption = "Descriptive statistics for ordinal items")
```

Additionally, we can plot the data.
The `ggplot2` function `geom_bar()` is useful for ordinal data;
`geom_density()` is useful for continuous data:

```{r, echo = TRUE, eval = FALSE}
names(df) <- paste0("u.", 1:4)
df_plot <- reshape(df, varying = names(df), direction = "long")
ggplot(df_plot, aes(x = u)) +
  geom_bar() +
  facet_wrap(~time)+
  theme_bw()
```
```{r, echo = FALSE, eval = TRUE}
names(df) <- paste0("Value.u", 1:4)
df_plot <- reshape(df, varying = names(df), direction = "long")
p <- ggplot(df_plot, aes(x = Value)) +
  geom_bar(na.rm = TRUE) +
  facet_wrap(~time)+
  theme_bw()
ggsave("plot_desc.svg", p, device = "svg", width = 100, height = 100, units = "mm")
knitr::include_graphics("plot_desc.svg")
```

As we can see, the `descriptives()` table provides invaluable information about the measurement level of the indicators,
which is used to specify the model correctly.
If these data had not been coded as ordinal factors,
these descriptive statistics would have revealed that each variable has only 3 unique values.
Attention should alo be paid to the pattern of
missingness, as discussed in the *Handling Missing Data* section.
The proportion of missing values is reported in the `"missing"` column.
If any variables had missing values, we would report an MCAR test with `mice::mcar()`,
and explain that missing data are accounted for using FIML.
In our example, we see that there are no missing values, hence we
proceed with our analysis.

In this case, no preprocessing is necessary:
all variables are on the same 3-point scale.
For an example on LCA with extremely skewed variables, see the `LCGA` vignette.

## Conducting Latent Class Analysis

Before we fit a series of LCA models, we set a random seed using
`set.seed()`.
This is important because there is some inherent randomness in the estimation procedure,
and using a seed ensures that we (and others) can exactly reproduce the results.

Next, we fit the LCA models.
As all variables are ordinal, we can use the convenience function
`tidySEM::mx_lca()`, which is a wrapper for the generic function `mx_mixture()` optimized for LCA with ordinal data.
Any mixture model can be specified through `mx_mixture()`.
At the time of writing, there are two other wrapper functions for special cases:
`mx_profiles()`, for latent profile analysis, and `mx_growth_mixture()`, for latent growth analysis and growth mixture models.
All of these functions have arguments `data` and number of `classes`.
All variables in `data` are included in the analysis,
so relevant variables must be selected first.
Below, we fit 1 to 4 class solutions:

```{r fitlca, eval = run_everything, echo = FALSE}
set.seed(123) # setting seed 
df2 <- mice::ampute(df, prop = .01)
names(df) <- paste0("u", 1:4)
res <- mx_lca(data = df, classes = 1:4, run = FALSE) # fitting LCA 1 to 4 class solutions
saveRDS(res, "res.RData")
```
```{r eval = FALSE, echo = TRUE}
set.seed(123)
res <- mx_lca(data = df, classes = 1:4)
table_fit(res)
```
```{r eval = FALSE, echo = FALSE}
res <- readRDS("res.RData")
```

In this example, all models converge without issues.
If this is not the case, however, we can try to aid convergence using `mxTryHardOrdinal()`, which expands the search for optimal parameter values for models with ordinal indicators.
It is part of the family of functions based on `mxTryHard()`.

## Class Enumeration

In class enumeration, we want to compare a sequence of LCA models fitted
to the data. To aid the process, we create a model fit table using
`tidySEM::table_fit()` with the results object as the input. As the
output contains a lot of information on each of the four fitted models,
we select a subset of helpful model fit indices and classification
diagnostics.

```{r fit_table, include = TRUE, eval=F}
fit_table <- table_fit(res) # model fit table
fit_table[ , c("Name", "LL", "Parameters", 
               "AIC", "BIC", "Entropy", 
               "prob_min", "prob_max", 
               "n_min", "n_max")] # our selection
```

Our selection of fit indices and classification diagnostics includes:

```{r, echo=F}
Selection <- c(
    "Name", "LL", "Parameters", 
    "AIC", "BIC", "prob_min", 
    "prob_max", "n_min", "n_max")

Description <- c(
    "Number of classes $K$",
    "Basic fit measure of the natural log of the likelihood of the data",
    "Total number of model parameters (in all classes)",
    "Relative fit measure: Akaike Information Criterion",
    "Relative fit measure: Bayesian Information Criterion",
    "Lowest average posterior class probability for class cases were assigned to",
    "Highest average posterior class probability for class cases were assigned to",
    "Proportion of cases in the smallest class based on posterior class probability",
    "Proportion of cases in the largest class based on posterior class probability")

selection <- cbind(Selection, Description)

knitr::kable(selection, format = 'pipe', 
             caption = "Selection of Fit Indices and 
                        Classification Diagnostics")
```


We discussed several possible strategies to select the final class
solution. Here, we apply our own.

To aid our interpretation of the results, we create an elbow plot
showing the trends in information criteria across four models.

```{r elbow_plot, include = TRUE, message=F, warning=F, eval=F}
library(tidyverse) # for data-wrangling
library(ggplot2) # for plots

elbow_plot <- fit_table[ , c("Name", "AIC", "BIC")] # extract ICs
elbow_plot <- pivot_longer(elbow_plot, cols = c("AIC", "BIC"), 
                           names_to = "IC", values_to = "Value") # to long format

ggplot(elbow_plot, aes(x = Name, y = Value, group = IC))+
  geom_point(aes(color = IC))+
  geom_line(aes(color = IC))+
  labs(title="Elbow Plot of Information Criteria per Class Solution", 
       x = "Class Solution", y = " Value")+
  scale_color_manual(name = "Information Criterion", 
                     values = c(AIC = 'blue', BIC = 'red'))+
  theme_minimal()

```

From the elbow plot, we see that AIC has a lower penalty for model
complexity. However, we are more interested in the BIC values, which are
similar for the one, two and three-class solutions, but the four-class
solution fits significantly worse. For this reason, we eliminate the
four-class solution from the selection process.

Then we examine the model fit table. As expected, the -2\*log likelihood
falls successively with each added class. As previously stated,
classification diagnostics should not be used for model selection, but
they can be used to disqualify certain solutions because they are
uninterpretable. We see that prob_min and n_min for the four-class solution is
low, knowing that this solution also has a high BIC, we disqualify this
solution.

Out of the remaining three solutions, we notice that entropy is the
highest for the three-class solution, and it has a satisfactory prob_min
and n_min. Based on this, we retain the three class solution in model
selection. Note that entropy for the one-class solution will always
equal to one, as it is 100% true that every case is in that class. Based
on the low entropy of the two-class solution, we eliminate this model.

Finally, when comparing the one and three-class solutions, we inspect
the information criteria. For BIC, the one-class solution fits better,
but the difference is marginal. AIC tells us that the added complexity
of having three classes still explains the data better than a one-class
solution. Therefore, we select the three-class solution as our
final-class solution.

## Interpreting the Final Class Solution

To aid our understanding of the final class solution, we use
`ggplot2::plot_prob()` with the results of the three-class model as the
input. The resulting graph shows response patterns on all the indicators
for each group.

If we want to know the probability of each response option's endorsement
for each class, we can use `tidySEM::table_prob`. These are thresholds
for ordinal dependent variables in the probability scale.

```{r plot_prob, include = TRUE, eval=F}
plot_prob(res[[3]]) # visualizing the response patterns for the final model
table_prob(res[[3]]) # tabulating the response patterns for the final model
```

In the plot, we can see the distributions of the response probabilities
on the indicators for each of the three classes. For instance, we see
that in Class 1 the most common response to u2 is 2, while in Class 2
and Class 3 this is 0. We can also see that response 1 is a rare
response not forming the majority in any class. Class 2 distinguishes
itself because the majority scores the response 0 category of u3 and u4,
while in Class 1 and 2 this is not the case. Class 3 distinguishes
itself because the most common response to u3 and u4 is 3.

We can also interpret the response patterns numerically. It is a matter of 
preferences on how to interpret these probabilities. Here is where you would
**name** each class, such that each response pattern is theoretically meaningful.

## Extracting Posterior Class Probabilities

Another step is to extract posterior class probabilities. This is done
by the use of `tidySEM::class_prob` with the results of the final class
solution as the input.

```{r extract_Post_Class_Prob, include = TRUE, eval=F}
probs <- class_prob(res[[3]]) # extracting the posterior class probabilities
probs$mostlikely.class # posterior probabilities by most likely class membership
probs$individual # individual posterior class probabilities
```
